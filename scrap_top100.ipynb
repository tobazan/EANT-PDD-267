{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.7 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "1cb3c4095699f2a8cd6a12889e7edb42215ac8b2afd459e3e81282dc1470f9b2"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# TAREA SCRAPING\n",
    "\n",
    "### Vamos a scrapear la lista de 100 libros mas vendidos de Cuspide\n",
    "\n",
    "---\n",
    "\n",
    "Arranco scrapeando los enlaces a cada uno de los 100 libros"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.cuspide.com/top100.aspx'\n",
    "cuspide = requests.get(url)\n",
    "soup = BeautifulSoup(cuspide.text, 'lxml')\n",
    "\n",
    "libros = soup.find_all('article')\n",
    "\n",
    "links = [libro.a.get('href') for libro in libros]"
   ]
  },
  {
   "source": [
    "---\n",
    "\n",
    "Voy a hacerle un request a cada enlace, para scrapear **titulo** y **precio** para luego guardar una lista de libros con url, titulo y precio\n",
    "\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultado = []\n",
    "\n",
    "for link in links:\n",
    "        url_libro = 'https://www.cuspide.com' + link\n",
    "        try:\n",
    "            dom = requests.get(url_libro)\n",
    "            s = BeautifulSoup(dom.text, 'lxml')\n",
    "        except:\n",
    "            url_libro = f'Error en request a: {url_libro}'\n",
    "        try:\n",
    "            titulo = s.find('li', attrs={'class':'active'}).a.getText()\n",
    "        except:\n",
    "            titulo = 'Error al scrapear'\n",
    "        try:\n",
    "            precio = s.find('meta', attrs={'itemprop':'price'}).get('content').split('.')\n",
    "            precio_ars = int(precio[0][1:] + precio[1][0:3])\n",
    "        except:\n",
    "            precio = '0'\n",
    "        \n",
    "        resultado.append([url_libro, titulo, precio_ars])"
   ]
  },
  {
   "source": [
    "---\n",
    "\n",
    "Una vez que tengo esa lista, abro conexion a la BD MySQL y creo una tabla para guardar la data scrpeada\n",
    "\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONEXION ABIERTA\n",
    "import mysql.connector\n",
    "cone = mysql.connector.connect(\n",
    "                  host = 'cloud.eant.tech',\n",
    "                  database = 'pdp_base007',\n",
    "                  user = 'pdp_usuario007',\n",
    "                  password = 'eantpass')\n",
    "cursor = cone.cursor()\n",
    "\n",
    "#CREAMOS TABLA\n",
    "tabla = 'top100cuspide'\n",
    "create_query = f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS `{tabla}`\n",
    "        ( `id` INT NOT NULL AUTO_INCREMENT , \n",
    "        `titulo` VARCHAR(200) NOT NULL , \n",
    "        `precio_ars` INT NOT NULL , \n",
    "        `url_libro` VARCHAR(200) NOT NULL ,\n",
    "        PRIMARY KEY (`id`)) \n",
    "        ENGINE = InnoDB;\n",
    "        \"\"\"\n",
    "cursor.execute(create_query)"
   ]
  },
  {
   "source": [
    "---\n",
    "\n",
    "Inserto un registro a la tabla por cada libro y por ultimo cierro la conexion\n",
    "\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in resultado:\n",
    "    #ITERA LA LISTA SCRAPEADA CREANDO UN ROW EN LA TABLA POR CADA LIBRO\n",
    "    insert_query = f\"\"\"\n",
    "        INSERT INTO top100cuspide (url_libro, titulo, precio_ars)\n",
    "        VALUES ('{row[0]}', '{row[1]}', '{row[2]}');\n",
    "        \"\"\"\n",
    "    \n",
    "    cursor.execute(insert_query)\n",
    "    cone.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CERRAMOS CONEXION   \n",
    "cursor.close()\n",
    "cone.close()"
   ]
  }
 ]
}